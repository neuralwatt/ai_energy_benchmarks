python concurrent_inference_load.py --model "deepseek-r1:8b-llama-distill-q8_0" --prompts prompts.csv --concurrency 1  --num-requests 1 --processes 1 --max-tokens 2000 --port 11434 --no-random --stream
 

